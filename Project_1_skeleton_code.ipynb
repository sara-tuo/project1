{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff    # For computing gradients using automatic differentiation\n",
    "using LinearAlgebra \n",
    "using Plots # For plots\n",
    "pyplot() # For plots\n",
    "#test\n",
    "\n",
    "## Functions to compute gradient, hessian and first derivative\n",
    "∇(f,x) = ForwardDiff.gradient(f, x);\n",
    "H(f,x)  = ForwardDiff.hessian(f, x);\n",
    "D(θ,λ) = ForwardDiff.derivative(θ, λ)\n",
    "\n",
    "## Select which line search method to use.\n",
    "@enum LS ARMIJO GOLDEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact line search : golden section\n",
    "### Parameters: \n",
    " θ: line search function\n",
    " \n",
    " a: initial lower bound\n",
    " \n",
    " b: initial upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function golden_ls(θ, a, b)\n",
    " \n",
    "    l  = 1e-7                     # Tolerance (length of uncertainty)\n",
    "    α  = 1/Base.MathConstants.φ   # φ = golden ratio. Here α ≈ 0.618\n",
    "    \n",
    "    λ  = a + (1-α)*(b - a)        # NOTE: We do not need to index a, b, λ, and μ like in the lecture 5 pseudocode\n",
    "    μ  = a + α*(b - a)            #       Instead, we can keep reusing and updating the same variables for notational convenience\n",
    "\n",
    "    θμ = θ(a + α*(b - a))         # Use this variable to compute function values Θ(μₖ₊₁) as in the pseudocode of Lecture 5\n",
    "    θλ = θ(a + (1 - α)*(b - a))   # Use this variable to compute function values Θ(λₖ₊₁) as in the pseudocode of Lecture 5\n",
    "    \n",
    "    ## TODO: Implement what should be inside the while loop of Golden Section method\n",
    "    while b - a > l\n",
    "        if θλ > θμ\n",
    "            a = λ\n",
    "            λ = μ\n",
    "            θλ = θμ\n",
    "            μ = a + α*(b - a)\n",
    "            θμ = θ(μ) \n",
    "        else\n",
    "            b = μ\n",
    "            μ = λ\n",
    "            θμ = θλ\n",
    "            λ  = a + (1-α)*(b - a)\n",
    "            θλ = θ(λ)\n",
    "        end      \n",
    "    end\n",
    "    \n",
    "    return (a + b)/2              # Finally, the function returns the centre point of the final interval\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inexact line search : Armijo rule\n",
    "### Parameters: \n",
    "\n",
    "θ: line search function\n",
    "\n",
    "λ: initial step size value (e.g. 1)\n",
    "\n",
    "α: slope reduction factor\n",
    "\n",
    "β: λ reduction factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Armijo_ls(θ, λ, α, β) \n",
    "    \n",
    "    θ₀  = θ(0)                  # Function value at zero\n",
    "    Dθ₀ = D(θ, 0)               # Derivative (slope) at zero   \n",
    "    \n",
    "    ## TODO: Implement what should be inside the while loop of Armijo method\n",
    "    while θ(λ) > θ₀ + α*λ*Dθ₀   # Check termination condition\n",
    " \n",
    "        ##tämä on minun\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return λ\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient (descent) method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO)\n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "\n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    " \n",
    " β: λ reduction factor for Armijo's method\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Gradient(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    # if we need to save the history of iterations \n",
    "    (flag == true) && (x_iter = zeros(N, length(x))) # Equivalent to if flag == true then x_iter = zeros(N, length(x)) end\n",
    "    \n",
    "    for k = 1:N               # NOTE: initial x should be set to x0    \n",
    "        \n",
    "        ∇f     = ∇(f, x)      # Gradient at iteration k  \n",
    "        norm∇f = norm(∇f)     # Norm of the gradient        \n",
    "        ∇f    /= norm∇f       # Normalized gradient\n",
    "        \n",
    "        if norm∇f < ε         # Stopping condition #1\n",
    "            \n",
    "            if flag == true \n",
    "                \n",
    "                return  (x_iter[1:k-1, :], k-1)    # Return  the history of cost, iterations            \n",
    "            else\n",
    "                \n",
    "                return (f(x), k-1)                 # Return cost and iterations            \n",
    "           \n",
    "            end        \n",
    "            \n",
    "        end\n",
    "        \n",
    "        ## TODO: set the Gradient Descent direction\n",
    "        d =   - (∇f / norm(∇f))                # Gradient method direction\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)\n",
    "        LS == ARMIJO && (λ = Armijo_ls(θ, λ, α, β))         # Calls Armijo method to compute optimal step size λ \n",
    "        LS == GOLDEN && (λ = golden_ls(θ, a, b))            # Calls Golden Section method to compute optimal step size λ  \n",
    "        ############ END LINE SEARCH ###############\n",
    "       \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x +             # Update solution\n",
    "        \n",
    "        (flag == true) && (x_iter[k, :] = x) # save the history if needed\n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        return (x_iter, N)  # Return the history of cost, iterations\n",
    "    else    \n",
    "        return (f(x), N)   # Return cost, iterations  \n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy Ball method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO)\n",
    "  \n",
    "  weight: Heavy ball weighting parameter\n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    " \n",
    " β: λ reduction factor for Armijo's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Heavy_ball(f, x, N, LS, weight, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x))) # if we need to save the history of iterations \n",
    "    \n",
    "    d = zeros(size(x)) \n",
    "    \n",
    "    for k = 1:N               # Main iteration loop\n",
    "        \n",
    "        ∇f     = ∇(f, x)      # Gradient at iteration k  \n",
    "        norm∇f = norm(∇f)     # Norm of the gradient\n",
    "        ∇f    /= norm∇f       # Normalized gradient\n",
    "        \n",
    "        if norm∇f < ε         # Stopping condition: norm of the gradient < tolerance\n",
    "            if flag == true\n",
    "                return (x_iter[1:k-1, :], k-1)  # Return  the history of cost, iterations\n",
    "            else \n",
    "                return (f(x), k-1)                 # Return cost and iterations\n",
    "            end\n",
    "        end     \n",
    "        \n",
    "        ## TODO: set the Heavy ball direction\n",
    "        d = \n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)                                   # Define the line search function \n",
    "        LS == ARMIJO && (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        LS == GOLDEN && (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ  \n",
    "        ############ END LINE SEARCH ###############\n",
    "              \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x =  x + \n",
    "        \n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "    \n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        return (x_iter, N)  # Return the history of cost, iterations\n",
    "    else    \n",
    "        return (f(x), N)    # Return cost, iterations  \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: starting point\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO) \n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    "  \n",
    " β: λ reduction factor for Armijo's method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Newton(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)  \n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x)) ) # if we need to save the hostiry of iterations\n",
    "    \n",
    "    for k = 1:N                    # NOTE: initial x should be set to x0     \n",
    "        ∇f = ∇(f, x)               # Gradient\n",
    "        \n",
    "        if norm(∇f) < ε            # Stopping condition #1\n",
    "            if flag == true\n",
    "                return  (x_iter[1 : k-1, :], k-1)  # Return  the history of cost, iterations\n",
    "            else \n",
    "                return (f(x), k-1)                 # Return cost and iterations\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        ## TODO: set the Newton's method direction\n",
    "        d = \n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)\n",
    "        LS == ARMIJO && (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        LS == GOLDEN && (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ  \n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x +                      # Move to a new point\n",
    "       \n",
    "        (flag == true) && ( x_iter[k, :] = x ) # save the history if needed\n",
    "        \n",
    "    end\n",
    "    \n",
    "    if flag == true\n",
    "        return (x_iter, N )  # Return the history of cost, iterations\n",
    "    else    \n",
    "        return ( f(x), N )   # Return cost, iterations  \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyden–Fletcher–Goldfarb–Shanno (BFGS) method \n",
    "### Parameters\n",
    "  f: function to minimize\n",
    "  \n",
    "  x: intial solution vector\n",
    "  \n",
    "  N: maximum number of iterations\n",
    "  \n",
    "  LS: line search method (GOLDEN or ARMIJO) \n",
    "  \n",
    "  flag: the indicator of output (true - if you need the history of the iterations, false - if you need just cost and number of iterations)\n",
    "  \n",
    "  ### Keywork arguments\n",
    "  \n",
    " ε: solution tolerance\n",
    " \n",
    " a: initial lower bound for Golden section method\n",
    "\n",
    " b: initial upper bound for Golden section method\n",
    " \n",
    " λ: initial step size value (e.g. 1) for Armijo's method\n",
    " \n",
    " α: slope reduction factor for Armijo's method\n",
    "  \n",
    " β: λ reduction factor for Armijo's method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !For the sake of efficiency, implement the version of BFGS that updates the inverse of the Hessian, which does not require inversion operations or the use of the backslash (\\\\) operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function BFGS(f, x, N, LS, flag; ε = tol, a = a₀, b = b₀, λ = λ₀, α = α₀, β = β₀)\n",
    "    \n",
    "    (flag == true) && (x_iter = zeros(N, length(x))) # if we need to save the hostiry of iterations\n",
    "    \n",
    "    n  = length(x)                # Number of variables\n",
    "    B  = Matrix(1.0I, n, n)       # Initial Hessian approximation is the identity\n",
    "    ∇f = ∇(f, x)                  # Initial gradient at x0\n",
    "    \n",
    "    for k = 1:N                   # NOTE: initial x should be set to x0\n",
    "        if norm(∇f) < ε          # Stopping condition #1\n",
    "            if flag == true\n",
    "                return  (x_iter[1 : k-1, :], k-1)  # Return  the history of cost, iterations\n",
    "            else \n",
    "                return (f(x), k-1)                 # Return cost and iterations\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        ## TODO:: set the BFGS method direction using the approximation for the inverse of the Hessian\n",
    "        d =                           # Direction dₖ\n",
    "        \n",
    "        ########## START LINE SEARCH ###############\n",
    "        θ(λ) = f(x + λ*d)\n",
    "        LS == ARMIJO && (λ = Armijo_ls(θ, λ, α, β))         # Call Armijo method to compute optimal step size λ \n",
    "        LS == GOLDEN && (λ = golden_ls(θ, a, b))            # Call Golden Section method to compute optimal step size λ \n",
    "        ############ END LINE SEARCH ###############\n",
    "        \n",
    "        p = λ*d                       # p = step size * direction\n",
    "        \n",
    "        ## TODO: Update the solution x at this iteration accordingly\n",
    "        x = x +                              # Update solution\n",
    "        \n",
    "        (flag == true) && (x_iter[k, :] = x) # save the history if needed \n",
    "        \n",
    "        ∇fn   = ∇(f, x)                      # New gradient\n",
    "        q     = ∇fn - ∇f                     # Update Gradient difference\n",
    "        ∇f    = ∇fn                          # Update Gradient for next iteration\n",
    "        \n",
    "        ## TODO: Update the approximation of the inverse of the Hessian\n",
    "        B = B + \n",
    "       \n",
    "    end\n",
    "    \n",
    "    if flag == true      \n",
    "        return (x_iter, N)  # Return the history of cost, iterations\n",
    "    else    \n",
    "        return (f(x), N)   # Return cost, iterations  \n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self examination \n",
    "### Test Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined parameters\n",
    "N   = 10000             # Number of iterations \n",
    "a₀  = 0.0               # initial lower bound for Golden section method\n",
    "b₀  = 10.0              # initial upper bound for Golden section method\n",
    "λ₀  = 2.0               # initial step size value for Armijo's method\n",
    "tol = 1e-5              # Solution tolerance\n",
    "\n",
    "# These are to be further set for the assignment\n",
    "α₀  = 0.01              # slope reduction factor for Armijo's method\n",
    "β₀  = 0.7               # λ reduction factor for Armijo's method\n",
    "heavy_ball_weight = 0.2 # Heavy ball weighting parameter\n",
    "\n",
    "# Function fot the test \n",
    "f(x) = exp(x[1] + 2*x[2] - 0.2) + exp(x[1] - 2*x[2] - 0.2) + exp(-x[1] - 0.2) \n",
    "\n",
    "# Starting point for f\n",
    "x = [-4.0, -2.0]      \n",
    "\n",
    "## TODO: Uncomment the line you need. \n",
    "# If your code is correct these should return the optimal point [-0.34657 0.0] and the optimal value 2.3157202\n",
    "\n",
    "# Test: just cost and iterations\n",
    "#@show (fgg, kgg) = Gradient(f, x, N, GOLDEN, false)\n",
    "#@time (fgg, kgg) = Gradient(f, x, N, GOLDEN, false)\n",
    "\n",
    "#@show (fga, kga) = Gradient(f, x, N, ARMIJO, false)\n",
    "#@time (fga, kga) = Gradient(f, x, N, ARMIJO, false)\n",
    "\n",
    "#@show (fhg, khg) = Heavy_ball(f, x, N, GOLDEN, heavy_ball_weight, false)\n",
    "#@time (fhg, khg) = Heavy_ball(f, x, N, GOLDEN, heavy_ball_weight, false)\n",
    "\n",
    "#@show (fha, kha) = Heavy_ball(f, x, N, ARMIJO, heavy_ball_weight, false);\n",
    "#@time (fha, kha) = Heavy_ball(f, x, N, ARMIJO, heavy_ball_weight, false);\n",
    "\n",
    "#@show (fng, kng) = Newton(f, x, N, GOLDEN, false)\n",
    "#@time (fng, kng) = Newton(f, x, N, GOLDEN, false)\n",
    "\n",
    "#@show (fna, kna) = Newton(f, x, N, ARMIJO, false)\n",
    "#@time (fna, kna) = Newton(f, x, N, ARMIJO, false)\n",
    "\n",
    "#@show (fbg, kbg) = BFGS(f, x, N, GOLDEN, false)\n",
    "#@time (fbg, kbg) = BFGS(f, x, N, GOLDEN, false)\n",
    "\n",
    "#@show (fba, kba) = BFGS(f, x, N, ARMIJO, false)\n",
    "#@time (fba, kba) = BFGS(f, x, N, ARMIJO, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint:\n",
    "If you want to measure how much time your function call takes, you can use the macro @time in front of the call. See http://www.pkofod.com/2017/04/24/timing-in-julia/ for more information on timing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of how to plot the function contour and one of the methods trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the history of the iterations for the Gradient method with Armijo rule as an inexact line search\n",
    "## TODO: Don't forget to set the last parameter to 'true' implying that you want to receive the history of the iterations as an output\n",
    "(xga, kga) = Gradient(f, x, N, ARMIJO, true)\n",
    "\n",
    "# Gather the history of the iterations for the Gradient method with Golden section rule as an exact line search\n",
    "## TODO: Don't forget to set the last parameter to 'true' implying that you want to receive the history of the iterations as an output\n",
    "(xgg, kgg) = Gradient(f, x, N, GOLDEN, true)\n",
    "\n",
    "# Defining the interval for the x1(x) and x2(y) axes\n",
    "x = range( -2.5, 2.5, length = 100)\n",
    "y = copy(x)\n",
    "\n",
    "# Plotting the contour of the function \n",
    "contour(x,y, (x,y) -> f([x,y]), \n",
    "    title  = \"Test function\",\n",
    "    levels = [0.0 + 5i for i = 1:10],\n",
    "    cbar = false,\n",
    "    clims = (0,50)\n",
    ")\n",
    "\n",
    "# Plotting the trajectory of the Gradient method with Armijo rule as an inexact line search\n",
    "plot!(xga[:,1], xga[:,2], label = \"Gradient (Armijo)\", marker=:x)\n",
    "\n",
    "# Plotting the trajectory of the Gradient method with Golden section rule as an exact line search\n",
    "plot!(xgg[:,1], xgg[:,2], label = \"Gradient (Golden)\", marker=:x)\n",
    "\n",
    "# Saving the figure as .pdf file if needed\n",
    "savefig(\"test_function_Gradient.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calls for tasks (1) - (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the parameters' values used for the experiments.\n",
    "Notice that some parameters must be set by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be set by you.\n",
    "## TODO: set the values for α₀, β₀, and heavy_ball_weight. \n",
    "α₀  = 0.01              # slope reduction factor for Armijo's method\n",
    "β₀  = 0.7               # λ reduction factor for Armijo's method\n",
    "heavy_ball_weight = 0.2 # Heavy ball weighting parameter\n",
    "\n",
    "# Predefined parameters (nothing to be changed from here onwards)\n",
    "N   = 10000             # Number of iterations \n",
    "a₀  = 0.0               # initial lower bound for Golden section method\n",
    "b₀  = 10.0              # initial upper bound for Golden section method\n",
    "λ₀  = 2.0               # initial step size value (e.g. 2) for Armijo's method\n",
    "tol = 1e-5              # Solution tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task (1)\n",
    "## TODO: Uncomment the line you need\n",
    "\n",
    "# Function for task (1)\n",
    "f1(x) = 2*(0.5x[1]^2 + 4x[2]^2) - 0.5*x[1]*x[2] \n",
    "\n",
    "# Starting point for f1\n",
    "x1 = [-10.0, 10.0]      \n",
    "\n",
    "# (1): just cost and iterations. \n",
    "#@show (f1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, false)\n",
    "#@time (f1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, false)\n",
    "\n",
    "#@show (f1ga, k1ga) = Gradient(f1, x1, N, ARMIJO, false)\n",
    "#@time (f1ga, k1ga) = Gradient(f1, x1, N, ARMIJO, false)\n",
    "\n",
    "#@show (f1hg, k1hg) = Heavy_ball(f1, x1, N, GOLDEN, heavy_ball_weight, false)\n",
    "#@time (f1hg, k1hg) = Heavy_ball(f1, x1, N, GOLDEN, heavy_ball_weight, false)\n",
    "\n",
    "#@show (f1ha, k1ha) = Heavy_ball(f1, x1, N, ARMIJO, heavy_ball_weight, false)\n",
    "#@time (f1ha, k1ha) = Heavy_ball(f1, x1, N, ARMIJO, heavy_ball_weight, false)\n",
    "\n",
    "\n",
    "# (1): history and iterations\n",
    "#(x1gg, k1gg) = Gradient(f1, x1, N, GOLDEN, true)\n",
    "#(x1ga, k1ga) = Gradient(f1, x1, N, ARMIJO, true)\n",
    "#(x1hg, k1hg) = Heavy_ball(f1, x1, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(x1ha, k1ha) = Heavy_ball(f1, x1, N, ARMIJO, heavy_ball_weight, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task (2)\n",
    "## TODO: Uncomment the line you need\n",
    "\n",
    "# Function for task (2)\n",
    "f2(x) = exp(x[1] + 2*x[2] - 0.1) + exp(x[1] - 2*x[2] - 0.1) + exp(-x[1] - 0.2) \n",
    "\n",
    "# Starting point for f2\n",
    "x2 = [-2.5, -3.5]      \n",
    "\n",
    "# (2): (just cost and iterations)\n",
    "#@show (f2gg, k2gg) = Gradient(f2, x2, N, GOLDEN, false)\n",
    "#@time (f2gg, k2gg) = Gradient(f2, x2, N, GOLDEN, false)\n",
    "\n",
    "#@show (f2ga, k2ga) = Gradient(f2, x2, N, ARMIJO, false)\n",
    "#@time (f2ga, k2ga) = Gradient(f2, x2, N, ARMIJO, false)\n",
    "\n",
    "#@show (f2hg, k2hg) = Heavy_ball(f2, x2, N, GOLDEN, heavy_ball_weight, false)\n",
    "#@time (f2hg, k2hg) = Heavy_ball(f2, x2, N, GOLDEN, heavy_ball_weight, false)\n",
    "\n",
    "#@show (f2ha, k2ha) = Heavy_ball(f2, x2, N, ARMIJO, heavy_ball_weight, false)\n",
    "#@time (f2ha, k2ha) = Heavy_ball(f2, x2, N, ARMIJO, heavy_ball_weight, false)\n",
    "\n",
    "#@show (f2ng, k2ng) = Newton(f2, x2, N, GOLDEN, false)\n",
    "#@time (f2ng, k2ng) = Newton(f2, x2, N, GOLDEN, false)\n",
    "\n",
    "#@show (f2na, k2na) = Newton(f2, x2, N, ARMIJO, false)\n",
    "#@time (f2na, k2na) = Newton(f2, x2, N, ARMIJO, false)\n",
    "\n",
    "#@show (f2bg, k2bg) = BFGS(f2, x2, N, GOLDEN, false)\n",
    "#@time (f2bg, k2bg) = BFGS(f2, x2, N, GOLDEN, false)\n",
    "\n",
    "#@show (f2ba, k2ba) = BFGS(f2, x2, N, ARMIJO, false)\n",
    "#@time (f2ba, k2ba) = BFGS(f2, x2, N, ARMIJO, false)\n",
    "\n",
    "# (2): (history and iterations)\n",
    "#(x2gg, k2gg) = Gradient(f2, x2, N, GOLDEN, true)\n",
    "#(x2ga, k2ga) = Gradient(f2, x2, N, ARMIJO, true)\n",
    "#(x2hg, k2hg) = Heavy_ball(f2, x2, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(x2ha, k2ha) = Heavy_ball(f2, x2, N, ARMIJO, heavy_ball_weight, true)\n",
    "#(x2ng, k2ng) = Newton(f2, x2, N, GOLDEN, true)\n",
    "#(x2na, k2na) = Newton(f2, x2, N, ARMIJO, true)\n",
    "#(x2bg, k2bg) = BFGS(f2, x2, N, GOLDEN, true)\n",
    "#(x2ba, k2ba) = BFGS(f2, x2, N, ARMIJO, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task (3)\n",
    "## TODO: Uncomment the line you need\n",
    "\n",
    "# Function for task (3)\n",
    "f3(x) = (x[1]^2 + x[2] - 10)^2 + (x[1] + x[2]^2 - 15)^2\n",
    "\n",
    "# Starting point for f3\n",
    "x3 = [-0.5, -2.0]     \n",
    "\n",
    "# (3): (just cost and iterations)\n",
    "#@show (f3gg, k3gg) = Gradient(f3, x3, N, GOLDEN, false)\n",
    "#@time (f3gg, k3gg) = Gradient(f3, x3, N, GOLDEN, false)\n",
    "\n",
    "#@show (f3ga, k3ga) = Gradient(f3, x3, N, ARMIJO, false)\n",
    "#@time (f3ga, k3ga) = Gradient(f3, x3, N, ARMIJO, false)\n",
    "\n",
    "#@show (f3hg, k3hg) = Heavy_ball(f3, x3, N, GOLDEN, heavy_ball_weight, false)\n",
    "#@time (f3hg, k3hg) = Heavy_ball(f3, x3, N, GOLDEN, heavy_ball_weight, true)\n",
    "\n",
    "#@show (f3ha, k3ha) = Heavy_ball(f3, x3, N, ARMIJO, heavy_ball_weight, false)\n",
    "#@time (f3ha, k3ha) = Heavy_ball(f3, x3, N, ARMIJO, heavy_ball_weight, false)\n",
    "\n",
    "#@show (f3ng, k3ng) = Newton(f3, x3, N, GOLDEN, false)\n",
    "#@time (f3ng, k3ng) = Newton(f3, x3, N, GOLDEN, true)\n",
    "\n",
    "#@show (f3na, k3na) = Newton(f3, x3, N, ARMIJO, false)\n",
    "#@time (f3na, k3na) = Newton(f3, x3, N, ARMIJO, false)\n",
    "\n",
    "#@show (f3bg, k3bg) = BFGS(f3, x3, N, GOLDEN, false)\n",
    "#@time (f3bg, k3bg) = BFGS(f3, x3, N, GOLDEN, true)\n",
    "\n",
    "#@show (f3ba, k3ba) = BFGS(f3, x3, N, ARMIJO, false)\n",
    "#@time (f3ba, k3ba) = BFGS(f3, x3, N, ARMIJO, false)\n",
    "\n",
    "# (3) (history and iterations)\n",
    "#(x3gg, k3gg) = Gradient(f3, x3, N, GOLDEN, true)\n",
    "#(x3ga, k3ga) = Gradient(f3, x3, N, ARMIJO, true)\n",
    "#(x3hg, k3hg) = Heavy_ball(f3, x3, N, GOLDEN, heavy_ball_weight, true)\n",
    "#(x3ha, k3ha) = Heavy_ball(f3, x3, N, ARMIJO, heavy_ball_weight, true)\n",
    "#(x3ng, k3ng) = Newton(f3, x3, N, GOLDEN, true)\n",
    "#(x3na, k3na) = Newton(f3, x3, N, ARMIJO, true)\n",
    "#(x3bg, k3bg) = BFGS(f3, x3, N, GOLDEN, true)\n",
    "#(x3ba, k3ba) = BFGS(f3, x3, N, ARMIJO, true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random # for generating the instances\n",
    "\n",
    "## Generate a random symmetric positive definite matrix\n",
    "## A ∈ ℜⁿˣⁿ and a random vector b ∈ ℜⁿ\n",
    "function generate_problem_data(n::Int, δ::Float64)\n",
    "    A = randn(n,n)                # Create random matrix\n",
    "    A = (A + A')/2                # Make A symmetric\n",
    "    if isposdef(A) == false       # Check if A is PD\n",
    "        λᵢ = eigmin(A)            # Minimum eigenvalue\n",
    "        A = A + (abs(λᵢ) + δ)*I   # Add λᵢ + δ to diagonal elements\n",
    "    end\n",
    "    @assert(isposdef(A))          # Final PD test\n",
    "    b = randn(n)                  # Create random vector b\n",
    "    return (A,b)                  # Resulting matrix A is PD\n",
    "end\n",
    "\n",
    "\n",
    "## Generate k test instances of dimension n\n",
    "function generate_instances(k::Int, n::Int, δ::StepRangeLen)\n",
    "    A = Dict{Int,Matrix{Float64}}()   # Store matrices A\n",
    "    b = Dict{Int,Vector{Float64}}()   # Store vectors  b\n",
    "    for i = 1:k\n",
    "        ## NOTE: Change δ between, e.g., δ ∈ [0.01, 1] to get different\n",
    "        ##       condition numbers for matrix A\n",
    "        (A[i], b[i]) = generate_problem_data(n, δ[i])\n",
    "    end\n",
    "    return (A, b)\n",
    "end\n",
    "\n",
    "Random.seed!(0)                             # Control randomness\n",
    "k = 100                                     # Number of intances to generate\n",
    "n = 150                                     # Dimension of PD matrix A ∈ ℜⁿˣⁿ\n",
    "δ1 = range(0.05, length = k, step = 0.05)   # Moderate condition numbers for matrices A\n",
    "δ2 = range(0.01, length = k, step = 0.01)   # Larger condition numbers for matrices A\n",
    "\n",
    "\n",
    "## Generate problem data with δ1 and δ2\n",
    "(A1, b1) = generate_instances(k, n, δ1)\n",
    "(A2, b2) = generate_instances(k, n, δ2)\n",
    "\n",
    "\n",
    "## Function to minimize with two different data\n",
    "f1(x,i) = (1/2)*dot(x, A1[i]*x) - dot(b1[i], x)\n",
    "f2(x,i) = (1/2)*dot(x, A2[i]*x) - dot(b2[i], x)\n",
    "\n",
    "## Optimal solution costs\n",
    "fopt = zeros(k,2)\n",
    "for i = 1:k\n",
    "    x1 = A1[i]\\b1[i]\n",
    "    x2 = A2[i]\\b2[i]\n",
    "    fopt[i,1] = f1(x1,i)\n",
    "    fopt[i,2] = f2(x2,i)\n",
    "end\n",
    "\n",
    "###### Preallocate data #######\n",
    "\n",
    "# Solution costs\n",
    "fval_gradient_golden   = zeros(k, 2)\n",
    "fval_gradient_armijo   = zeros(k, 2)\n",
    "fval_heavy_ball_golden = zeros(k, 2)\n",
    "fval_heavy_ball_armijo = zeros(k, 2)\n",
    "fval_newton_golden     = zeros(k, 2)\n",
    "fval_newton_armijo     = zeros(k, 2)\n",
    "fval_bfgs_golden       = zeros(k, 2)\n",
    "fval_bfgs_armijo       = zeros(k, 2)\n",
    "\n",
    "# Solution times\n",
    "time_gradient_golden   = zeros(k, 2)\n",
    "time_gradient_armijo   = zeros(k, 2)\n",
    "time_heavy_ball_golden = zeros(k, 2)\n",
    "time_heavy_ball_armijo = zeros(k, 2)\n",
    "time_newton_golden     = zeros(k, 2)\n",
    "time_newton_armijo     = zeros(k, 2)\n",
    "time_bfgs_golden       = zeros(k, 2)\n",
    "time_bfgs_armijo       = zeros(k, 2)\n",
    "\n",
    "# Number of iterations\n",
    "iter_gradient_golden   = zeros(Int, k, 2)\n",
    "iter_gradient_armijo   = zeros(Int, k, 2)\n",
    "iter_heavy_ball_golden = zeros(Int, k, 2)\n",
    "iter_heavy_ball_armijo = zeros(Int, k, 2)\n",
    "iter_newton_golden     = zeros(Int, k, 2)\n",
    "iter_newton_armijo     = zeros(Int, k, 2)\n",
    "iter_bfgs_golden       = zeros(Int, k, 2)\n",
    "iter_bfgs_armijo       = zeros(Int, k, 2)\n",
    "\n",
    "# Solution status\n",
    "stat_gradient_golden   = fill(false, k, 2)\n",
    "stat_gradient_armijo   = fill(false, k, 2)\n",
    "stat_heavy_ball_golden = fill(false, k, 2)\n",
    "stat_heavy_ball_armijo = fill(false, k, 2)\n",
    "stat_newton_golden     = fill(false, k, 2)\n",
    "stat_newton_armijo     = fill(false, k, 2)\n",
    "stat_bfgs_golden       = fill(false, k, 2)\n",
    "stat_bfgs_armijo       = fill(false, k, 2)\n",
    "\n",
    "ns  = 8                          # Number of solvers (methods) to compare\n",
    "np  = k                          # Number of problems to solve\n",
    "computing_time = zeros(np,ns,2)  # Computing times for each problem/method.\n",
    "\n",
    "# Resetting the parameters for the performance profiles\n",
    "# !Do not change these parameters hereafrer\n",
    "                                 \n",
    "N   = 10000                      # Number of iterations\n",
    "x₀  = ones(n)                    # Starting point\n",
    "tol = 1e-5                       # Solution tolerance\n",
    "a₀  = 0.0                        # initial lower bound for Golden section method\n",
    "b₀  = 10.0                       # initial upper bound for Golden section method\n",
    "λ₀  = 1.5                        # initial step size value for Armijo's method\n",
    "\n",
    "α₀  = 0.01                       # slope reduction factor for Armijo's method\n",
    "β₀  = 0.7                        # λ reduction factor for Armijo's method\n",
    "heavy_ball_weight = 0.5          # Heavy ball weighting parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tini = time()\n",
    "## Go through all instances for both sets of data\n",
    "for j = 1:2 \n",
    "    for i = 1:k\n",
    "  \n",
    "        @info \"Solving problem $(j), run $(i)/$(k).\"        \n",
    "        ## Function to minimize\n",
    "        g1(x) = f1(x,i)\n",
    "        g2(x) = f2(x,i)\n",
    "    \n",
    "        ## Gradient + Golden\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Gradient(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Gradient(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_gradient_golden[i, j] = fvalue       # Objective value\n",
    "        time_gradient_golden[i, j] = soltime      # Solution time\n",
    "        iter_gradient_golden[i, j] = numiter      # Iteration count\n",
    "        stat_gradient_golden[i, j] = status       # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 1, j] = soltime : computing_time[i, 1, j] = Inf    \n",
    "    \n",
    "        ## Gradient + Armijo\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Gradient(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Gradient(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime            # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_gradient_armijo[i, j] = fvalue     # Objective value\n",
    "        time_gradient_armijo[i, j] = soltime    # Solution time\n",
    "        iter_gradient_armijo[i, j] = numiter    # Iteration count\n",
    "        stat_gradient_armijo[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 2, j] = soltime : computing_time[i, 2, j] = Inf  \n",
    "    \n",
    "        ## Heavy ball + Golden\n",
    "        starttime = time()                      # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Heavy_ball(g1, x₀, N, GOLDEN, heavy_ball_weight, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Heavy_ball(g2, x₀, N, GOLDEN, heavy_ball_weight, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_heavy_ball_golden[i, j] = fvalue     # Objective value\n",
    "        time_heavy_ball_golden[i, j] = soltime    # Solution time\n",
    "        iter_heavy_ball_golden[i, j] = numiter    # Iteration count\n",
    "        stat_heavy_ball_golden[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 3, j] = soltime : computing_time[i, 3, j] = Inf  \n",
    "    \n",
    "        ## Heavy ball + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Heavy_ball(g1, x₀, N, ARMIJO, heavy_ball_weight, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Heavy_ball(g2, x₀, N, ARMIJO, heavy_ball_weight, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_heavy_ball_armijo[i, j] = fvalue     # Objective value\n",
    "        time_heavy_ball_armijo[i, j] = soltime    # Solution time\n",
    "        iter_heavy_ball_armijo[i, j] = numiter    # Iteration count\n",
    "        stat_heavy_ball_armijo[i, j] = status     # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 4, j] = soltime : computing_time[i, 4, j] = Inf  \n",
    "\n",
    "        ## Newton + Golden\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Newton(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Newton(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_newton_golden[i, j] = fvalue         # Objective value\n",
    "        time_newton_golden[i, j] = soltime        # Solution time\n",
    "        iter_newton_golden[i, j] = numiter        # Iteration count\n",
    "        stat_newton_golden[i, j] = status         # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 5, j] = soltime : computing_time[i, 5, j] = Inf\n",
    "    \n",
    "        ## Newton + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = Newton(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = Newton(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_newton_armijo[i, j] = fvalue         # Objective value\n",
    "        time_newton_armijo[i, j] = soltime        # Solution time\n",
    "        iter_newton_armijo[i, j] = numiter        # Iteration count\n",
    "        stat_newton_armijo[i, j] = status         # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 6, j] = soltime : computing_time[i, 6, j] = Inf    \n",
    "        \n",
    "        ## BFGS + Golden\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = BFGS(g1, x₀, N, GOLDEN, false)\n",
    "        else\n",
    "            (fvalue, numiter) = BFGS(g2, x₀, N, GOLDEN, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_bfgs_golden[i, j] = fvalue           # Objective value\n",
    "        time_bfgs_golden[i, j] = soltime          # Solution time\n",
    "        iter_bfgs_golden[i, j] = numiter          # Iteration count\n",
    "        stat_bfgs_golden[i, j] = status           # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 7, j] = soltime : computing_time[i, 7, j] = Inf\n",
    "    \n",
    "        ## BFGS + Armijo\n",
    "        starttime = time()                        # Start timer\n",
    "        if j == 1\n",
    "            (fvalue, numiter) = BFGS(g1, x₀, N, ARMIJO, false)\n",
    "        else\n",
    "            (fvalue, numiter) = BFGS(g2, x₀, N, ARMIJO, false)\n",
    "        end\n",
    "        soltime = time() - starttime              # Solution time\n",
    "        status = abs(fvalue - fopt[i, j]) <= tol  # Check if solved or not\n",
    "        fval_bfgs_armijo[i, j] = fvalue           # Objective value\n",
    "        time_bfgs_armijo[i, j] = soltime          # Solution time\n",
    "        iter_bfgs_armijo[i, j] = numiter          # Iteration count\n",
    "        stat_bfgs_armijo[i, j] = status           # Solution status\n",
    "        ## Set solution time accordingly\n",
    "        status == true ? computing_time[i, 8, j] = soltime : computing_time[i, 8, j] = Inf\n",
    "    end    \n",
    "end\n",
    "tend = time() - tini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j = 1:2\n",
    "    ###### Plot performance profiles ######\n",
    "    computing_time_min = minimum(computing_time[:, :, j], dims = 2)    # Minimum time for each instance\n",
    "    performance_ratios = computing_time[:, :, j] ./ computing_time_min # Compute performance ratios\n",
    "\n",
    "    τ = sort(unique(performance_ratios))  # Sort the performance ratios in increasing order\n",
    "    τ[end] == Inf && pop!(τ)  # Remove the Inf element if it exists\n",
    "\n",
    "    ns = 8                    # Number of solvers\n",
    "    np = k                    # Number of problems\n",
    "\n",
    "    ρS = Dict()               # Compute cumulative distribution functions\n",
    "    for i = 1:ns              # for performance ratios\n",
    "        ρS[i] = [sum(performance_ratios[:,i] .<= τi) / np for τi in τ]\n",
    "    end\n",
    "\n",
    "    # Plot performance profiles\n",
    "    labels = [\"Gradient (Exact)\", \"Gradient (Armijo)\", \"Heavy ball (Exact)\", \"Heavy ball (Armijo)\",\n",
    "              \"Newton (Exact)\", \"Newton (Armijo)\", \"BFGS (Exact)\", \"BFGS (Armijo)\"]\n",
    "\n",
    "    styles = [:solid, :dash, :dot, :dashdot, :solid, :dash, :solid, :dash,]\n",
    "    plot(xscale = :log2,  \n",
    "         yscale = :none,\n",
    "         xlim   = (1, maximum(τ)),\n",
    "         ylim   = (0, 1),\n",
    "         xlabel = \"τ\",\n",
    "         ylabel = \"P(performance_ratios ≤ τ : 1 ≤ s ≤ np)\",\n",
    "         title  = \"Perfomance plot (condition $(j))\",\n",
    "         yticks = 0.0:0.1:1.0,\n",
    "         size   = (1200,800),\n",
    "         reuse  = false,\n",
    "         tickfontsize   = 8,\n",
    "         legendfontsize = 10,\n",
    "         guidefontsize  = 10,\n",
    "         grid = true)\n",
    "    for i = 1:ns\n",
    "      plot!(τ, ρS[i], label = labels[i], seriestype = :steppre, linewidth = 2, line = styles[i])\n",
    "    end\n",
    "    savefig(\"performance_plot_$(j).pdf\")\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
